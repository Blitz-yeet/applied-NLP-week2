{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65cb4e7",
   "metadata": {},
   "source": [
    "# 5) Collocation Network\n",
    "\n",
    "This notebook is part of **Applied NLP – Session 2: Phrases & Collocations**.\n",
    "\n",
    "Overview:\n",
    "- Visualize word collocations as a network graph where nodes are words and edges are bigram co-occurrences.\n",
    "- Identify hub words (high degree) that co-occur frequently with many other words.\n",
    "- Filter edges by frequency threshold to focus on strong collocations and reveal phrase structure.\n",
    "\n",
    "Learning objectives:\n",
    "- Represent bigram data as a network graph using NetworkX.\n",
    "- Compute and visualize largest connected components to see collocation clusters.\n",
    "- Interpret node degree as a measure of word centrality in phrasal patterns.\n",
    "- Use network visualization to explore stylistic and thematic phrase structure.\n",
    "\n",
    "Quick start:\n",
    "1. Edit the `CONFIG` dictionary in the next code cell to point to your two plain-text books.\n",
    "2. Adjust `min_ngram_count` to control edge density (higher = sparser graph, clearer hubs).\n",
    "3. (Optional) Toggle `use_stopwords` to remove function words and focus on content-word collocations.\n",
    "4. Run cells from top to bottom. The main outputs are saved to `../results/`.\n",
    "\n",
    "Prerequisites:\n",
    "- A Python environment with requirements.txt packages installed (pandas, matplotlib, networkx).\n",
    "- The text files for the two works placed in `../data/`.\n",
    "\n",
    "Notes and tips:\n",
    "- The notebook uses the same robust preprocessing as notebooks 1-2 (strip_gutenberg, normalize quotes, etc.).\n",
    "- Network graph: each word is a node; each bigram (a, b) creates an edge between nodes a and b weighted by co-occurrence count.\n",
    "- Self-loops (edges from a word to itself) are filtered out for clarity.\n",
    "- Largest connected component is extracted to avoid isolated word pairs cluttering the visualization.\n",
    "- Node size scales with degree (number of connections); hub words appear larger.\n",
    "- Edge width scales with weight (bigram count); frequent collocations appear thicker.\n",
    "- Spring layout: positions nodes to minimize edge crossings and reveal structure.\n",
    "- Consider removing stopwords first or filtering to content-words (nouns, verbs, adjectives) for cleaner networks.\n",
    "- Compare per-book networks by running the same code on `text1` and `text2` separately to see shifts in collocation patterns.\n",
    "\n",
    "**Goal:** Build and visualize a collocation network to explore phrase structure and identify hub words in your two selected works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609e5a1a",
   "metadata": {},
   "source": [
    "## 0. Setup & Configuration\n",
    "\n",
    "- Fill the `CONFIG` paths for your two books (plain text).\n",
    "- Toggle stopwords and thresholds as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65431f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Imports & Config =====\n",
    "import re, os, math, json, collections\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "CONFIG = {\n",
    "    \"book1_path\": \"../data/PetSemetary.txt\",   # Pet Sematary\n",
    "    \"book2_path\": \"../data/TheShining.txt\",    # The Shining\n",
    "    \"language\": \"en\",\n",
    "    \"use_stopwords\": False,                    # toggle if you want\n",
    "    \"min_ngram_count\": 5,                      # bigram frequency threshold\n",
    "    \"top_k\": 20\n",
    "}\n",
    "\n",
    "# Unicode-aware token regex: words with optional internal ' or -\n",
    "WORD_RE = re.compile(r\"[^\\W\\d_]+(?:[-'][^\\W\\d_]+)*\", flags=re.UNICODE)\n",
    "\n",
    "# Optional stopword set\n",
    "STOPWORDS = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11547f59",
   "metadata": {},
   "source": [
    "## 1. Load & Normalize Text\n",
    "\n",
    "- Fix hyphenated line breaks (e.g., end-of-line hyphens).\n",
    "- Normalize whitespace.\n",
    "- Lowercase consistently.\n",
    "\n",
    "Our books are a part of Project Gutenberg, which means there are some extra texts in each txt file to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0187e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(p: str) -> str:\n",
    "    \"\"\"Load text file as UTF-8, ignoring bad bytes.\"\"\"\n",
    "    return Path(p).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    \"\"\"Simple normalization for local TXT files (not Gutenberg).\"\"\"\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    # normalize curly apostrophes to ASCII '\n",
    "    t = t.replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
    "    # join hyphenated line breaks (e.g., \"won-\\n derful\" → \"wonderful\")\n",
    "    t = re.sub(r\"-\\s*\\n\", \"\", t)\n",
    "    # normalize whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t.strip()\n",
    "\n",
    "# Load + normalize\n",
    "text1 = normalize_text(load_text(CONFIG[\"book1_path\"]))  # Pet Sematary\n",
    "text2 = normalize_text(load_text(CONFIG[\"book2_path\"]))  # The Shining\n",
    "\n",
    "# Tokenize\n",
    "tokens1 = WORD_RE.findall(text1.lower())\n",
    "tokens2 = WORD_RE.findall(text2.lower())\n",
    "\n",
    "if CONFIG[\"use_stopwords\"]:\n",
    "    tokens1 = [t for t in tokens1 if t not in STOPWORDS]\n",
    "    tokens2 = [t for t in tokens2 if t not in STOPWORDS]\n",
    "\n",
    "# Optional: prune junk 1–2 letter tokens to keep network clean\n",
    "keep_1 = {\"a\", \"i\"}\n",
    "keep_2 = {\n",
    "    \"of\",\"to\",\"in\",\"on\",\"by\",\"an\",\"or\",\"as\",\"is\",\"it\",\"we\",\"us\",\"he\",\"me\",\"my\",\n",
    "    \"so\",\"be\",\"do\",\"no\",\"at\",\"up\",\"if\",\"go\",\"am\",\"oh\"\n",
    "}\n",
    "\n",
    "tokens1 = [\n",
    "    t for t in tokens1\n",
    "    if (len(t) > 2)\n",
    "       or (len(t) == 1 and t in keep_1)\n",
    "       or (len(t) == 2 and t in keep_2)\n",
    "]\n",
    "tokens2 = [\n",
    "    t for t in tokens2\n",
    "    if (len(t) > 2)\n",
    "       or (len(t) == 1 and t in keep_1)\n",
    "       or (len(t) == 2 and t in keep_2)\n",
    "]\n",
    "\n",
    "# Combined corpus for a single network (you can also build two separate ones if you want)\n",
    "tokens = tokens1 + tokens2\n",
    "\n",
    "print(\"Token counts after normalization + pruning:\")\n",
    "print(\"Pet Sematary:\", len(tokens1))\n",
    "print(\"The Shining :\", len(tokens2))\n",
    "print(\"Combined    :\", len(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b1399b",
   "metadata": {},
   "source": [
    "## 2. Build Bigram Graph\n",
    "\n",
    "- Nodes: words\n",
    "- Edges: bigrams (weight = co-occurrence count)\n",
    "- Filter edges by `min_ngram_count` for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import networkx as nx\n",
    "\n",
    "# --- Choose which tokens you want the network for ---\n",
    "# Option A: combined network for both books\n",
    "network_tokens = tokens\n",
    "\n",
    "# Option B: per-book networks (uncomment one if you want that instead)\n",
    "# network_tokens = tokens1   # Pet Sematary only\n",
    "# network_tokens = tokens2   # The Shining only\n",
    "\n",
    "# --- Optional: extra pruning before building the graph ---\n",
    "keep_1 = {\"a\", \"i\"}\n",
    "keep_2 = {\n",
    "    \"of\",\"to\",\"in\",\"on\",\"by\",\"an\",\"or\",\"as\",\"is\",\"it\",\"we\",\"us\",\"he\",\"me\",\"my\",\n",
    "    \"so\",\"be\",\"do\",\"no\",\"at\",\"up\",\"if\",\"go\",\"am\",\"oh\"\n",
    "}\n",
    "\n",
    "clean_tokens = [\n",
    "    t for t in network_tokens\n",
    "    if (len(t) > 2)\n",
    "       or (len(t) == 1 and t in keep_1)\n",
    "       or (len(t) == 2 and t in keep_2)\n",
    "]\n",
    "\n",
    "# --- Build bigram counts ---\n",
    "bigrams_counts = Counter(zip(clean_tokens, clean_tokens[1:]))\n",
    "min_c = CONFIG[\"min_ngram_count\"]\n",
    "\n",
    "# edges = (word_a, word_b, weight)\n",
    "edges = [\n",
    "    (a, b, c)\n",
    "    for (a, b), c in bigrams_counts.items()\n",
    "    if c >= min_c and a != b\n",
    "]\n",
    "\n",
    "# --- Build graph ---\n",
    "G = nx.Graph()\n",
    "for a, b, c in edges:\n",
    "    G.add_edge(a, b, weight=c)\n",
    "\n",
    "# Keep largest connected component for readability\n",
    "if G.number_of_nodes() > 0:\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    H = G.subgraph(largest_cc).copy()\n",
    "else:\n",
    "    H = G\n",
    "\n",
    "print(f\"Full graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "print(f\"Largest component: {H.number_of_nodes()} nodes, {H.number_of_edges()} edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb26a26",
   "metadata": {},
   "source": [
    "## 3. Visualize Graph (spring layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a643a0",
   "metadata": {},
   "source": [
    "## 3a. Filtered Network (Top Hubs Only)\n",
    "\n",
    "For a clearer visualization, we aggressively filter to show only:\n",
    "- **Top hub words** (highest degree nodes)\n",
    "- **Strong connections** between these hubs only (edge weight ≥ 2× min_ngram_count)\n",
    "\n",
    "This reveals the core collocation structure without overcrowding. You can adjust:\n",
    "- `top_n_hubs` - how many hub words to show (default: 20)\n",
    "- `min_edge_weight` - minimum connection strength to display (default: 2× min_ngram_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66454f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy: Show only strongest edges between high-degree nodes\n",
    "# This creates a much cleaner, more readable network\n",
    "\n",
    "# 1. Select top hubs by degree\n",
    "top_n_hubs = 50  # start with top 20 hub words\n",
    "deg = dict(H.degree())\n",
    "top_hubs = set(sorted(deg, key=deg.get, reverse=True)[:top_n_hubs])\n",
    "\n",
    "# 2. Filter to only include edges between these hubs with strong connections\n",
    "# Only keep edges where BOTH nodes are hubs AND edge weight is high\n",
    "edge_weights = [(u, v, d['weight']) for u, v, d in H.edges(data=True)]\n",
    "edge_weights.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Keep only edges where both nodes are in top hubs and weight is above threshold\n",
    "min_edge_weight = CONFIG[\"min_ngram_count\"] * 2  # stricter threshold for cleaner graph\n",
    "filtered_edges = [\n",
    "    (u, v, w) for u, v, w in edge_weights \n",
    "    if u in top_hubs and v in top_hubs and w >= min_edge_weight\n",
    "]\n",
    "\n",
    "# 3. Build clean filtered graph\n",
    "H_filtered = nx.Graph()\n",
    "for u, v, w in filtered_edges:\n",
    "    H_filtered.add_edge(u, v, weight=w)\n",
    "\n",
    "print(f\"Filtered network: {H_filtered.number_of_nodes()} nodes, {H_filtered.number_of_edges()} edges\")\n",
    "\n",
    "# 4. Visualize - much cleaner now!\n",
    "fig_filtered = plt.figure(figsize=(14,10))\n",
    "pos = nx.spring_layout(H_filtered, k=0.8, iterations=100, seed=42)\n",
    "deg_filtered = dict(H_filtered.degree())\n",
    "wts_filtered = [H_filtered[u][v][\"weight\"] for u,v in H_filtered.edges()]\n",
    "\n",
    "if wts_filtered and H_filtered.number_of_nodes() > 0:\n",
    "    wmin, wmax = min(wts_filtered), max(wts_filtered)\n",
    "    ew_filtered = [1 + 4*(w - wmin)/(wmax - wmin + 1e-9) for w in wts_filtered]\n",
    "    \n",
    "    # Draw nodes with size based on degree\n",
    "    node_sizes = [200 + 50*deg_filtered[n] for n in H_filtered]\n",
    "    nx.draw_networkx_nodes(H_filtered, pos, node_size=node_sizes, alpha=0.85, node_color='lightblue', edgecolors='steelblue', linewidths=2)\n",
    "    nx.draw_networkx_edges(H_filtered, pos, width=ew_filtered, alpha=0.5, edge_color='gray')\n",
    "    \n",
    "    # Label all nodes - now readable!\n",
    "    nx.draw_networkx_labels(H_filtered, pos, font_size=12, font_weight='bold')\n",
    "    \n",
    "    plt.title(f\"Collocation Network - Top Hub Words (min edge weight ≥ {min_edge_weight})\")\n",
    "    plt.axis(\"off\"); plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(f\"Graph too sparse. Try lowering min_edge_weight (currently {min_edge_weight}) or increase top_n_hubs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f46fe",
   "metadata": {},
   "source": [
    "Can you now filter for stopwords and look again at the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c487a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_network = plt.figure(figsize=(10, 8))\n",
    "pos = nx.spring_layout(H, k=0.25, iterations=50, seed=42)\n",
    "deg_full = dict(H.degree())\n",
    "wts = [H[u][v][\"weight\"] for u, v in H.edges()]\n",
    "\n",
    "if wts:\n",
    "    wmin, wmax = min(wts), max(wts)\n",
    "    ew = [\n",
    "        0.5 + 2.5 * (w - wmin) / (wmax - wmin + 1e-9)\n",
    "        for w in wts\n",
    "    ]\n",
    "else:\n",
    "    ew = []\n",
    "\n",
    "nx.draw_networkx_nodes(\n",
    "    H, pos,\n",
    "    node_size=[24 + 6 * deg_full[n] for n in H],\n",
    "    alpha=0.85\n",
    ")\n",
    "nx.draw_networkx_edges(\n",
    "    H, pos,\n",
    "    width=ew,\n",
    "    alpha=0.35\n",
    ")\n",
    "\n",
    "# label only top hubs to avoid clutter\n",
    "top_nodes = sorted(deg_full, key=deg_full.get, reverse=True)[:20]\n",
    "nx.draw_networkx_labels(\n",
    "    H, pos,\n",
    "    labels={n: n for n in top_nodes},\n",
    "    font_size=9\n",
    ")\n",
    "\n",
    "plt.title(f\"Collocation Network – Largest Component (min bigram count ≥ {min_c})\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe91c2c",
   "metadata": {},
   "source": [
    "## 4. Notes\n",
    "\n",
    "- Consider removing stopwords or filtering to content-words first.\n",
    "- Compare per-book networks to see shifts in phrase structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee527f7",
   "metadata": {},
   "source": [
    "## 5. Reflection (Answer in your repo's README or below)\n",
    "\n",
    "- Which results matched your reading intuition?\n",
    "- What surprised you?\n",
    "- If you toggled preprocessing (stopwords on/off), what changed?\n",
    "- Compare across the two works: are the patterns stable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f59f5ad",
   "metadata": {},
   "source": [
    "## 6. Export (tables/figures)\n",
    "\n",
    "This cell saves outputs into the `../results/` folder so you can add them to your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d52b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"../results\").mkdir(exist_ok=True)\n",
    "\n",
    "try:\n",
    "    fig_network.savefig(\n",
    "        \"../results/collocation_network_full.png\",\n",
    "        dpi=200,\n",
    "        bbox_inches=\"tight\"\n",
    "    )\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if fig_filtered is not None:\n",
    "    try:\n",
    "        fig_filtered.savefig(\n",
    "            \"../results/collocation_network_filtered.png\",\n",
    "            dpi=200,\n",
    "            bbox_inches=\"tight\"\n",
    "        )\n",
    "    except Exception:\n",
    "        try:\n",
    "            plt.savefig(\n",
    "                \"../results/collocation_network_filtered.png\",\n",
    "                dpi=200,\n",
    "                bbox_inches=\"tight\"\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(\"Saved collocation network figures (where possible) into ../results/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
